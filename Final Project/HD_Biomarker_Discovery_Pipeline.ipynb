{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning-Based Transcriptomic Biomarker Discovery in Huntington's Disease\n",
    "\n",
    "**Project:** Discovery of Key Biomarkers for Huntington's Disease Using Meta-Analysis and Machine Learning\n",
    "**Data Source:** GSE64810 (Pre-processed & Filtered)\n",
    "**Models:** Random Forest (Feature Selection) & Support Vector Machine (Classification)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "Import necessary libraries and upload the `HD_ML_Ready_Data.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "# Setup plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"Please upload 'HD_ML_Ready_Data.csv'\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Check if file is uploaded\n",
    "filename = list(uploaded.keys())[0]\n",
    "print(f\"Loaded file: {filename}\")\n",
    "\n",
    "data = pd.read_csv(filename)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "Encoding the target variable and splitting the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Features (Genes) and Target\n",
    "X = data.drop('Target_Class', axis=1)\n",
    "y = data['Target_Class']\n",
    "\n",
    "# Encode Target (Control -> 0, HD -> 1)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "class_names = le.classes_\n",
    "print(f\"Class Mapping: {dict(zip(class_names, le.transform(class_names)))}\")\n",
    "\n",
    "# Split Data (70% Train, 30% Test) - Stratified to maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.3, stratify=y_encoded, random_state=42\n",
    ")\n",
    "\n",
    "# Scale data for SVM (SVM requires feature scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")\n",
    "print(f\"Number of Features (Genes): {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "Training Random Forest (for feature importance) and SVM (for high accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Random Forest ---\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# --- SVM (Linear Kernel) ---\n",
    "# probability=True is needed for ROC-AUC\n",
    "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Models trained successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation and Visualization\n",
    "Generating predictions, confusion matrices, and ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_prob_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "y_prob_svm = svm_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Metrics Function ---\n",
    "def print_metrics(name, y_true, y_pred, y_prob):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n--- {name} Results ---\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"ROC-AUC: {auc:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    return confusion_matrix(y_true, y_pred)\n",
    "\n",
    "cm_rf = print_metrics(\"Random Forest\", y_test, y_pred_rf, y_prob_rf)\n",
    "cm_svm = print_metrics(\"SVM\", y_test, y_pred_svm, y_prob_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot Confusion Matrices ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "axes[0].set_title('Random Forest Confusion Matrix')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "\n",
    "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Greens', ax=axes[1], \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "axes[1].set_title('SVM Confusion Matrix')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Plot ROC Curves ---\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)\n",
    "fpr_svm, tpr_svm, _ = roc_curve(y_test, y_prob_svm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_score(y_test, y_prob_rf):.2f})', lw=2)\n",
    "plt.plot(fpr_svm, tpr_svm, label=f'SVM (AUC = {roc_auc_score(y_test, y_prob_svm):.2f})', lw=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance (Biomarker Identification)\n",
    "Visualizing the top 20 genes that the Random Forest model used to make decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1][:20] # Top 20 features\n",
    "\n",
    "top_genes = X.columns[indices]\n",
    "top_importances = importances[indices]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=top_importances, y=top_genes, palette='viridis')\n",
    "plt.title('Top 20 Candidate Biomarkers (Random Forest Importance)')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Gene ID')\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 Genes:\")\n",
    "for gene, score in zip(top_genes[:10], top_importances[:10]):\n",
    "    print(f\"{gene}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save and Download Models\n",
    "We will save the trained models, the scaler (crucial for SVM), and the list of feature names (crucial for ensuring input consistency). We'll zip them together for a single download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for artifacts\n",
    "!mkdir -p hd_models\n",
    "\n",
    "# 1. Save Random Forest Model\n",
    "joblib.dump(rf_model, 'hd_models/rf_model.pkl')\n",
    "\n",
    "# 2. Save SVM Model\n",
    "joblib.dump(svm_model, 'hd_models/svm_model.pkl')\n",
    "\n",
    "# 3. Save Scaler (Required for SVM inference)\n",
    "joblib.dump(scaler, 'hd_models/scaler.pkl')\n",
    "\n",
    "# 4. Save Feature Names (To ensure future input order matches training)\n",
    "feature_names = list(X.columns)\n",
    "joblib.dump(feature_names, 'hd_models/feature_names.pkl')\n",
    "\n",
    "# 5. Save Label Encoder (To decode 0/1 back to Control/HD)\n",
    "joblib.dump(le, 'hd_models/label_encoder.pkl')\n",
    "\n",
    "print(\"Artifacts saved locally in 'hd_models/' folder.\")\n",
    "\n",
    "# Zip the folder\n",
    "!zip -r hd_models.zip hd_models\n",
    "\n",
    "# Download the zip file\n",
    "files.download('hd_models.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference Demo\n",
    "This section demonstrates how to load the saved models and run predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hd_status(new_data_df, model_type='svm'):\n",
    "    \"\"\"\n",
    "    Runs inference on new data.\n",
    "    Args:\n",
    "        new_data_df (pd.DataFrame): Dataframe with Gene IDs as columns.\n",
    "        model_type (str): 'svm' or 'rf'\n",
    "    Returns:\n",
    "        Prediction string (HD or Control)\n",
    "    \"\"\"\n",
    "    # Load artifacts\n",
    "    try:\n",
    "        rf_loaded = joblib.load('hd_models/rf_model.pkl')\n",
    "        svm_loaded = joblib.load('hd_models/svm_model.pkl')\n",
    "        scaler_loaded = joblib.load('hd_models/scaler.pkl')\n",
    "        features_loaded = joblib.load('hd_models/feature_names.pkl')\n",
    "        le_loaded = joblib.load('hd_models/label_encoder.pkl')\n",
    "    except FileNotFoundError:\n",
    "        return \"Error: Model files not found. Run the training steps first.\"\n",
    "    \n",
    "    # Validate Features\n",
    "    # Ensure new_data has the exact same columns in same order\n",
    "    try:\n",
    "        new_data_df = new_data_df[features_loaded]\n",
    "    except KeyError:\n",
    "        missing = set(features_loaded) - set(new_data_df.columns)\n",
    "        return f\"Error: Input data is missing genes: {list(missing)[:5]}...\"\n",
    "\n",
    "    # Prediction logic\n",
    "    if model_type == 'svm':\n",
    "        # SVM requires scaling\n",
    "        data_scaled = scaler_loaded.transform(new_data_df)\n",
    "        pred_idx = svm_loaded.predict(data_scaled)\n",
    "        prob = svm_loaded.predict_proba(data_scaled)[:, 1]\n",
    "    else:\n",
    "        # RF does not require scaling\n",
    "        pred_idx = rf_loaded.predict(new_data_df)\n",
    "        prob = rf_loaded.predict_proba(new_data_df)[:, 1]\n",
    "        \n",
    "    # Decode prediction\n",
    "    prediction = le_loaded.inverse_transform(pred_idx)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Prediction': prediction,\n",
    "        'Probability_HD': prob\n",
    "    })\n",
    "    return results\n",
    "\n",
    "# --- Test Inference using a sample from our test set ---\n",
    "# (Simulating 'new' data by taking 2 rows from X_test)\n",
    "sample_input = X_test.iloc[:2].copy()\n",
    "print(\"\\nRunning Inference on Sample Data:\")\n",
    "print(predict_hd_status(sample_input, model_type='svm'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}